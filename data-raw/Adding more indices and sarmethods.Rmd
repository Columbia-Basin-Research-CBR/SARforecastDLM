---
title: "AnotherMarkdownToCleanUp"
output: html_document
date: "2024-05-08"
---

Objective: Add back in removed code with different SAR methods, indices, and potentially passage type and rear type.

Current plan: 
SAR to include: Scheuerell and Williams (2005), CBR DART SAR, CJS estimated survival (Gosselin et al 2019?), and ?
Indices to include: CUI (Bakun index), CUTI, BEUTI?, Northern Copepod Biomass Index
Months to include: April
Passage type to include: All- combined
Rear type to include: Natural-origin


```{r load_libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(MARSS)
library(here)
library(rerddap) #import data from ERDDAP
```
# sar-raw-data

# Northern copepod biomass data: https://www.fisheries.noaa.gov/data-tools/newport-line-northern-copepod-biomass-data ; update with [NOAA](https://www.fisheries.noaa.gov/west-coast/science-data/local-biological-indicators#copepod-biodiversity) referenced data via [Hooff and Peterson, 2006](https://www.fisheries.noaa.gov/west-coast/science-data/local-biological-indicators#copepod-biodiversity); or ask jenn for where to source data

```{r load_cope_data}

#Northern copepod biomass data
cope<-read_csv(here("data-raw", "northern_copepod_biomass_newportOR.csv")) 

cope<-cope %>% 
  filter(Month == 4) %>% 
  group_by(Year) %>% 
  summarise(value = mean(NORTHERN)) %>% 
  select(Year, value) %>% 
  mutate(index = "NCBI")

#load existing sar_raw_data
load(here("data", "sar_raw_data.rda")) #load existing data

## load the salmon sar from MARSS
data(SalmonSurvCUI, package = "MARSS")

#extract sar and year
df<-SalmonSurvCUI %>% 
  select(year, logit.s)

#join sar and year with copepod data
df.cope<-df %>% inner_join( cope, by = c("year" = "Year"))
  
#join sar/cope data with existing sar_raw_data
sar_raw_data_updated<- sar_raw_data %>% 
  bind_rows(df.cope) %>% 
  mutate(sar.method = "Scheuerell and Williams (2005)")

```


# Pull in additional years of CUI
```{r}
### ERDAPP, NOAA upwelling indice CUI
# Set the URL of the ERDDAP server
url <- "https://upwell.pfeg.noaa.gov/erddap/"

# Get information about the erdCUTI dataset
info <- info(datasetid = "erdUI45mo", url = url)

# Download the dataset
dat <- griddap(info,
               time = c("1964-01-01T00:00:00Z", "2024-02-15T00:00:00Z"), #currently pulling all data to date (adjust as needed)
               latitude = c(45,45)
)

# format time and lat/long
df.CUI<-dat$data %>% 
  mutate(time.day = lubridate::ymd_hms(time),
         year = year(time.day),
         month = month(time.day),
         day = day(time.day), 
         adjlong = longitude-360) #set to -180 to 180

## only include april CUI from ERDAPP data (for now)
df.CUI<-df.CUI %>% 
  filter(month == 4) %>% 
  select(year, upwelling_index) 
```


# pull in additional years of CUTI
```{r}

#load CUTI
# Set the URL of the ERDDAP server
#url <- "https://upwell.pfeg.noaa.gov/erddap/"

# Get information about the erdCUTI dataset
info <- info(datasetid = "erdCUTImonthly", url = url)


# Download the dataset
dat <- griddap(info,
               time = c("1988-01-15T00:00:00Z", "2024-02-15T00:00:00Z"), #currently pulling all data to date
               latitude = c(45,45)
)


df.CUTI<-dat$data %>% 
  mutate(time.day = lubridate::ymd_hms(time),
         year = year(time.day),
         month = month(time.day),
         day = day(time.day)) %>% 
  filter(month == 4) %>% 
  select(year, CUTI)  %>% 
  rename(value = "CUTI") %>% 
  mutate(index = "CUTI")



# df.cuti<-df %>% inner_join( df.CUTI, by = "year")
#   
# #join data - returns year and sar repeated for cui and cuti
# sar_raw_data<- SalmonSurvCUI %>% 
#  rename("value" = CUI.apr) %>% 
#   mutate(index = "CUI") %>% 
#   bind_rows(df.cuti)

```


# CBR SAR data
```{r}
df.cbr<-read_csv(here("data-raw/SAR", "LGRtoLGA_allpass_W_spsu_ch.csv"))
df.cbr<-df.cbr[-c(24:31),] #drop CBR notes 
df.cbr <-  df.cbr %>% mutate(year = as.integer(year)) %>%   filter( between(year, 2002, 2020)) #2021 does not have ocean4 reported--keep to 2020?


df.cbr<-df.cbr %>% 
  select(year, meanSAR) %>% 
  mutate( logit.s = qlogis(meanSAR/100)) %>% #must divide meanSAR /100 to get 0 to 1 and then apply logit/transform
  select(-meanSAR) 

#CUI
df.cbr.cui<-df.cbr %>% 
  inner_join(df.CUI, by = "year") %>% 
  rename("value" = upwelling_index)  %>%
  mutate(index = "CUI",
         sar.method = "DART")
#CUTI
df.cbr.cuti<-df.cbr %>% 
  inner_join(df.CUTI, by = "year") %>% 
  mutate(sar.method = "DART")

#NCBI
df.cbr.ncbi<-df.cbr %>% 
  inner_join(cope, by = c("year"= "Year")) %>% 
  mutate(sar.method = "DART")

#join all indices
df.cbr<-rbind(df.cbr.cui, df.cbr.cuti, df.cbr.ncbi) 


#add to sar_raw_updated with DART sar method with CUI,CUTI, NCBI
sar_raw_data_updated<-rbind(sar_raw_data_updated, df.cbr)
```

# CJS estimated survival

```{r}
#add fake data until figured out
str(sar_raw_data_updated)
df.cjs <- expand.grid(
  year = 1994:2019,
  logit.s = rnorm(26)
)

#CUI
df.cjs.cui<- df.cjs %>% 
  inner_join(df.CUI, by = "year") %>% 
  rename("value" = upwelling_index)  %>%
  mutate(index = "CUI",
         sar.method = "CJS")
#CUTI
df.cjs.cuti<-df.cjs %>% 
  inner_join(df.CUTI, by = "year") %>% 
  mutate(sar.method = "CJS")

#NCBI
df.cjs.ncbi<-df.cjs %>% 
  inner_join(cope, by = c("year"= "Year")) %>% 
  mutate(sar.method = "CJS")

#join all indices
df.cjs<-rbind(df.cjs.cui, df.cjs.cuti, df.cjs.ncbi) 


#add to sar_raw_updated with DART sar method with CUI,CUTI, NCBI
sar_raw_data_updated<-rbind(sar_raw_data_updated, df.cjs)
```


# save updated sar_raw_data

```{r}
sar_raw_data_updated %>% 
  group_by(sar.method, index) %>% 
  summarise(n_distinct(year))

#save updated sar_raw_data
usethis::use_data(sar_raw_data_updated, overwrite = TRUE)
```

---


# base_plot_data


```{r}

# Define the function
run_model_and_forecast <- function(data) {
  ## get time indices
  years <- data$year
  ## number of years of data
  TT <- length(years)
  ## get response variable: logit(survival)
  dat <- matrix(data$logit.s, nrow = 1)

  ## get predictor variable
  index <- data$value

  ## ## ## z-score the upwelling index
  index_z <- matrix((index - mean(index)) / sqrt(var(index)), nrow = 1)

  index_z_train <- index_z[1:TT]

  ## number of regr params (slope + intercept) = 2
  m <- dim(index_z)[1] + 1


  ### build DLM

  ## for process eqn
  B <- diag(m)                        ## 2x2; Identity
  U <- matrix(0, nrow = m, ncol = 1)  ## 2x1; both elements = 0
  Q <- matrix(list(0), m, m)          ## 2x2; all 0 for now
  diag(Q) <- c("q.alpha", "q.beta")   ## 2x2; diag = (q1,q2)

  ## for observation eqn
  Z <- array(NA, c(1, m, TT))  ## NxMxT; empty for now
  Z[1,1,] <- rep(1, TT)        ## Nx1; 1's for intercept
  Z[1,2,] <- index_z_train             ## Nx1; predictor variable
  A <- matrix(0)               ## 1x1; scalar = 0
  R <- matrix("r")             ## 1x1; scalar = r

  ## only need starting values for regr parameters
  inits_list <- list(x0 = matrix(c(0, 0), nrow = m))

  ## list of model matrices & vectors
  mod_list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)


  ### fit DLM
  # set.seed(1234)
  ## fit univariate DLM
  dlm_train <- MARSS::MARSS(dat, inits = inits_list, model = mod_list)


  # #pull indice with full data z score
  # index_z_test <- index_z[(TT+1)] #get 1 CUI ahead

  # #newdata z array
  # Z_test <- array(NA, c(1, m, 3))  ## NxMxT; empty for now
  # Z_test[1,1,] <- rep(1, 3)        ## Nx1; 1's for intercept
  # Z_test[1,2,] <- index_z_test

  ## forecast
  forecast_df<-MARSS::forecast(dlm_train, h = 1, type = "ytT", interval = "confidence")

  #extract forecast
  forecast_df_pred<- forecast_df$pred
  return(forecast_df_pred)
}

# Get the unique combinations of index and sar.method
combinations <- unique(sar_raw_data_updated[, c("index", "sar.method")])


# Initialize an empty list to store the forecasts
forecasts <- list()

# Loop over each combination
for(i in seq_len(nrow(combinations))) {
  # Subset the data for the current combination
  data_subset <- sar_raw_data_updated[sar_raw_data_updated$index == combinations$index[i] & 
                                      sar_raw_data_updated$sar.method == combinations$sar.method[i], ]
  
  # Run the model and get the forecast
  forecast <- run_model_and_forecast(data_subset)
  
  # Store the forecast in the list
  forecasts[[i]] <- forecast
}

# Combine all forecasts into one data frame
all_forecasts <- do.call(rbind, forecasts)
```


```{r}
for i in 
fct_forecast_df<-function(data, years_selected, index_selected){

  # full_data<- data %>%
  #   dplyr::filter(dplyr::between(year, min(year), 2005),
  #                 index == index_selected)
  # 
  # train_data<- full_data %>%
  #   dplyr::filter(dplyr::between(year, min(year),max(years_selected)),
  #                 index == index_selected)
  # 
  # # test_data<- full_data %>%
  # # dplyr::filter(between(year,2000 +1, 2005),#max(years_selected)
  # #               index == "CUI")#index_selected)

  ## get time indices
  years <- data$year
  ## number of years of data
  TT <- length(years)
  ## get response variable: logit(survival)
  dat <- matrix(data$logit.s, nrow = 1)

  ## get predictor variable
  index <- data$value

  ## ## ## z-score the upwelling index
  index_z <- matrix((index - mean(index)) / sqrt(var(index)), nrow = 1)

  index_z_train <- index_z[1:TT]

  ## number of regr params (slope + intercept) = 2
  m <- dim(index_z)[1] + 1


  ### build DLM

  ## for process eqn
  B <- diag(m)                        ## 2x2; Identity
  U <- matrix(0, nrow = m, ncol = 1)  ## 2x1; both elements = 0
  Q <- matrix(list(0), m, m)          ## 2x2; all 0 for now
  diag(Q) <- c("q.alpha", "q.beta")   ## 2x2; diag = (q1,q2)

  ## for observation eqn
  Z <- array(NA, c(1, m, TT))  ## NxMxT; empty for now
  Z[1,1,] <- rep(1, TT)        ## Nx1; 1's for intercept
  Z[1,2,] <- index_z_train             ## Nx1; predictor variable
  A <- matrix(0)               ## 1x1; scalar = 0
  R <- matrix("r")             ## 1x1; scalar = r

  ## only need starting values for regr parameters
  inits_list <- list(x0 = matrix(c(0, 0), nrow = m))

  ## list of model matrices & vectors
  mod_list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)


  ### fit DLM
  # set.seed(1234)
  ## fit univariate DLM
  dlm_train <- MARSS::MARSS(dat, inits = inits_list, model = mod_list)


  # #pull indice with full data z score
  # index_z_test <- index_z[(TT+1)] #get 1 CUI ahead

  # #newdata z array
  # Z_test <- array(NA, c(1, m, 3))  ## NxMxT; empty for now
  # Z_test[1,1,] <- rep(1, 3)        ## Nx1; 1's for intercept
  # Z_test[1,2,] <- index_z_test

  ## forecast
  forecast_df<-MARSS::forecast(dlm_train, h = 1, type = "ytT", interval = "confidence")

  #extract forecast
  forecast_df_pred<- forecast_df$pred


  #wrangle for plot
  forecast_df_raw<-forecast_df_pred%>%
    dplyr::mutate(dplyr::across(c(,3:9), ~stats::plogis(.) * 100)) %>%
    dplyr::rename("fore_CI_95_upper" = `Hi 95`,
           "fore_CI_95_lower" = `Lo 95`) %>%
    dplyr::mutate(year = (min(years):max(years+1)),
           index = index_selected,
           sar.method = "Scheuerell and Williams (2005)",
           dataset = "select_forecast",
           rear_type = "Natural-origin",
           pass_type = "All") %>%
    dplyr::inner_join(dplyr::select(full_data, value, year), by = "year") %>% ##join index value
    dplyr::select(year, value, y, estimate, se, fore_CI_95_lower, fore_CI_95_upper,sar.method,index, rear_type, pass_type, dataset)


  return(forecast_df_raw)
}
```


 
