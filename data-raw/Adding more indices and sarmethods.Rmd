---
title: "AnotherMarkdownToCleanUp"
output: html_document
date: "2024-05-08"
---

Objective: Add back in removed code with different SAR methods, indices, and potentially passage type and rear type.

Current plan: 
SAR to include: Scheuerell and Williams (2005), CBR DART SAR, CJS estimated survival (Gosselin et al 2019?), and ?
Indices to include: CUI (Bakun index), CUTI, BEUTI?, Northern Copepod Biomass Index
Months to include: April
Passage type to include: All- combined
Rear type to include: Natural-origin


```{r load_libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(MARSS)
library(here)
library(rerddap) #import data from ERDDAP
library(readxl) #import excel files--NCBI
```
# sar-raw-data

As of 6/20/24 adding NCBI back into DLM as place holder for ICPB data. NCBI is publicly available whereas ICPB is not. 



# Northern copepod biomass data: include?
https://www.fisheries.noaa.gov/data-tools/newport-line-northern-copepod-biomass-data ; update with [NOAA](https://www.fisheries.noaa.gov/west-coast/science-data/local-biological-indicators#copepod-biodiversity) referenced data via [Hooff and Peterson, 2006](https://www.fisheries.noaa.gov/west-coast/science-data/local-biological-indicators#copepod-biodiversity);

```{r load_cope_data}

#Northern copepod biomass data
#NCBI<-read_csv(here("data-raw", "northern_copepod_biomass_newportOR.csv")) 

url<-"https://www.fisheries.noaa.gov/s3/2023-10/TempOxyNoSo-10232023.xlsx"

# Download the file to a temporary location
temp_file <- tempfile(fileext = ".xlsx")
download.file(url, temp_file, mode = "wb")

ncbi_raw<-read_excel(temp_file)



df_NCBI<-ncbi_raw %>% 
  filter(Month == 4) %>% #select certain months? Look into 
  group_by(Year) %>% 
  summarise(value = mean(NORTHERN)) %>% 
  select("year" = Year, value) %>% 
  mutate(index = "NCBI")

#load existing sar_raw_data
load(here("data", "sar_raw_data_updated.rda")) #load existing data

# ## load the salmon sar from MARSS
# data(SalmonSurvCUI, package = "MARSS") #only 9 years of overlap-- not including in DLM analysis



#extract sar and yea

#join sar and year with copepod data
df.cope<-df %>% inner_join( df_NCBI, by = "year")
  
#join sar/cope data with existing sar_raw_data
sar_raw_data_updated<- sar_raw_data_updated %>% 
  bind_rows(df.cope) %>% 
  mutate(sar.method = "Scheuerell and Williams (2005)")

```


## Index of Coastal Prey Biomass (ICPB) 

- Indice is of Winter (January, February, March) Ichthyoplankton 

- shared with JG via Elizabeth Daly - NOAA Affiliate see [Winter Ichthyoplankton via fisheries.noaa.gov](https://www.fisheries.noaa.gov/west-coast/science-data/2023-summary-ocean-ecosystem-indicators#winter-ichthyoplankton:~:text=at%20nearshore%20stations.-,Winter%20Ichthyoplankton,-The%202023%20winter). 

- Data in-house 1998 to 2019, email for more up to date data. 

- Since starts in 1998, will not include with SW SAR method, could look to older salmon prey biomass index (pre-ICPB). 

```{r load_ICPB_data}

#Northern copepod biomass data
df_ICPB<-read_csv(here("data-raw", "IndexOfCoastalPreyBiomass.csv")) 

# ## load the salmon sar from MARSS
# data(SalmonSurvCUI, package = "MARSS")

```


# Pull in additional years of CUI
```{r}
### ERDAPP, NOAA upwelling indice CUI
# Set the URL of the ERDDAP server
url <- "https://upwell.pfeg.noaa.gov/erddap/"

# Get information about the erdCUTI dataset
info <- info(datasetid = "erdUI45mo", url = url)

# Download the dataset
dat <- griddap(info,
               time = c("1964-01-01T00:00:00Z", "2024-04-15T00:00:00Z"), #currently pulling all data to date (adjust as needed)
               latitude = c(45,45)
)

# format time and lat/long
df.CUI<-dat$data %>% 
  mutate(time.day = lubridate::ymd_hms(time),
         year = year(time.day),
         month = month(time.day),
         day = day(time.day), 
         adjlong = longitude-360) #set to -180 to 180

## only include april CUI from ERDAPP data (for now)
df.CUI<-df.CUI %>% 
  filter(month == 4) %>% 
  select(year, upwelling_index) %>% 
    rename("value" = upwelling_index)  %>%
  mutate(index = "CUI")
```


# pull in additional years of CUTI
```{r}

#load CUTI
# Set the URL of the ERDDAP server
#url <- "https://upwell.pfeg.noaa.gov/erddap/"

# Get information about the erdCUTI dataset
info <- info(datasetid = "erdCUTImonthly", url = url)


# Download the dataset
dat <- griddap(info,
               time = c("1988-01-15T00:00:00Z", "2024-04-15T00:00:00Z"), #currently pulling all data to date
               latitude = c(45,45)
)


df.CUTI<-dat$data %>% 
  mutate(time.day = lubridate::ymd_hms(time),
         year = year(time.day),
         month = month(time.day),
         day = day(time.day)) %>% 
  filter(month == 4) %>% 
  select(year, CUTI)  %>% 
  rename(value = "CUTI") %>% 
  mutate(index = "CUTI")

```




#SAR methods

# CBR SAR data
```{r}
df.cbr.raw<-read_csv(here("data-raw/SAR", "LGRtoLGA_allpass_W_spsu_ch.csv"))
df.cbr.raw<-df.cbr.raw[-c(24:31),] #drop CBR notes 
df.cbr.raw <-  df.cbr.raw %>% mutate(year = as.integer(year)) %>%   filter( between(year, 2002, 2020)) #2021 does not have ocean4 reported--keep to 2020?


df.cbr<-df.cbr.raw %>% 
  select(year, meanSAR) %>% 
  mutate( logit.s = qlogis(meanSAR/100)) %>% #must divide meanSAR /100 to get 0 to 1 and then apply logit/transform
  select(-meanSAR) %>% 
  mutate(sar.method = "DART")


#CUI
df.cbr.cui<-df.cbr %>% inner_join(df.CUI, by = "year")

#CUTI
df.cbr.cuti<-df.cbr %>% inner_join(df.CUTI, by = "year")

#ICPB
df.cbr.icpb<-df.cbr %>% inner_join(df_ICPB, by = c("year"= "year")) 
#NCBI
df.cbr.ncbi<-df.cbr %>%   
  add_row(year = 2021, logit.s = NA, sar.method = "DART") %>%  #add 2021 with NA
  inner_join(df_NCBI, by = "year") 


#join all indices
df.cbr<-rbind(df.cbr.cui, df.cbr.cuti, df.cbr.icpb) 


#load existing sar_raw_data
load(here("data", "sar_raw_data.rda")) #load existing data

sar_raw_data_updated<-sar_raw_data %>% 
  mutate(sar.method = "Scheuerell and Williams (2005)")

#add to sar_raw_updated with DART sar method with CUI,CUTI, NCBI
sar_raw_data_updated<-rbind(sar_raw_data_updated, df.cbr)
```

# CJS estimated survival

```{r}
# #add fake data until figured out
# str(sar_raw_data_updated)
# df.cjs <- data.frame(
#   year = 1994:2019,
#   logit.s = rnorm(26),
#   sar.method = "CJS"
# )

df_cjs_raw<-read_csv(here("data-raw/SAR", "LGR_LGA_surv.csv")) #adding CJS sourced code and adding additional empty year 2020 -see cjs-model-code-no-covariates.R for code

df.cjs <- df_cjs_raw %>% 
  select(year, median) %>% 
  mutate(logit.s = qlogis(median/100)) %>% #requires   mutate(s = plogis(logit.s)*100) %>% #convert back to probability
  select(-median) %>% 
  mutate(sar.method = "CJS") %>% 
  add_row(year = 2020, logit.s = NA, sar.method = "CJS") #add 2020 with NA

sar<-sar_raw_data_updated %>% 
  filter(sar.method != "CJS")

#CUI
df.cjs.cui<- df.cjs %>% inner_join(df.CUI, by = "year") 
#CUTI
df.cjs.cuti<-df.cjs %>% inner_join(df.CUTI, by = "year")
#ICPB
df.cjs.icpb<-df.cjs %>% inner_join(df_ICPB, by = "year") #keep 2020
#NCBI
df.cjs.ncbi<-df.cjs %>% inner_join(df_NCBI, by = "year") #keep 2020


#join all indices
df.cjs<-rbind(df.cjs.cui, df.cjs.cuti, df.cjs.icpb) 

df.cjs <- df.cjs %>% 
  add_row(year = 2020, logit.s = NA, sar.method = "CJS", value = NA, index = "ICPB") #add 2020 for ICPB


#add to sar_raw_updated with DART sar method with CUI,CUTI, NCBI
sar_raw_data_updated<-rbind(sar_raw_data, df.cjs.ncbi)


##add ncbi to sar
sar_raw_data<- rbind(sar_raw_data, df.cjs.ncbi, df.cbr.ncbi)

```


# save updated sar_raw_data

```{r}
sar_raw_data%>% 
  group_by(sar.method, index) %>% 
  summarise(n_distinct(year))

#save updated sar_raw_data
usethis::use_data(sar_raw_data, overwrite = TRUE)

```

6/4/14, to use forecast method need to add additional year of indices to sar_raw_data. saved a new version
```{r appended_additional_year, eval=FALSE}
sar_raw_data_updated %>% 
  group_by(index, sar.method) %>% 
  summarise(min(year),
            max(year))

add_years<-data.frame(
  year = c(2020, 2021, 2006,2020, 2021, 2006 ),
  logit.s = NA,
  value = c(16, 30, 1, .255,.381,.361), #pulled values manually
  index = c("CUI", "CUI", "CUI", "CUTI", "CUTI", "CUTI"),
  sar.method = c("CJS", "DART", "Scheuerell and Williams (2005)", "CJS", "DART", "Scheuerell and Williams (2005)")
)

#oops need to add on ICPB blank years too--update if new data comes in
add_ICPB_years<-data.frame(
    year = c(2020, 2020),
  logit.s = NA,
  value = NA,
  index = c("ICPB", "ICPB"),
  sar.method = c("CJS", "DART")
)
 
 sar_raw_data_updated<- sar_raw_data_updated %>% 
    bind_rows(add_ICPB_years)
 
 #save updated sar_raw_data
usethis::use_data(sar_raw_data_updated, overwrite = TRUE)
```

---



# base_plot_data


```{r}

# Define the function
run_model_and_forecast <- function(data) {
  
  
  
  train.data<- data %>% 
    filter(!is.na(logit.s))
  
  test.data <- data %>%
    dplyr::filter(is.na(logit.s))
  
  
  ## get time indices
  years <- train.data$year
  ## number of years of data
  TT <- length(years)
  ## get response variable: logit(survival)
  dat <- matrix(train.data$logit.s, nrow = 1)

  ## get predictor variable
  index <- train.data$value

  ## ## ## z-score the upwelling index
  index_z <- matrix((index - mean(index)) / sqrt(var(index)), nrow = 1)

  # index_z_train <- index_z[1:TT]

  ## number of regr params (slope + intercept) = 2
  m <- dim(index_z)[1] + 1


  ### build DLM

  ## for process eqn
  B <- diag(m)                        ## 2x2; Identity
  U <- matrix(0, nrow = m, ncol = 1)  ## 2x1; both elements = 0
  Q <- matrix(list(0), m, m)          ## 2x2; all 0 for now
  diag(Q) <- c("q.alpha", "q.beta")   ## 2x2; diag = (q1,q2)

  ## for observation eqn
  Z <- array(NA, c(1, m, TT))  ## NxMxT; empty for now
  Z[1,1,] <- rep(1, TT)        ## Nx1; 1's for intercept
  Z[1,2,] <- index_z             ## Nx1; predictor variable
  A <- matrix(0)               ## 1x1; scalar = 0
  R <- matrix("r")             ## 1x1; scalar = r

  ## only need starting values for regr parameters
  inits_list <- list(x0 = matrix(c(0, 0), nrow = m))

  ## list of model matrices & vectors
  mod_list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)


  ### fit DLM
  # set.seed(1234)
  ## fit univariate DLM
  dlm<- MARSS::MARSS(dat, inits = inits_list, model = mod_list)
  
  
  ## forecast
  ## get list of Kalman filter output
  kf_out <- MARSSkfss(dlm)


  ## forecasts of regr parameters; 2xT matrix - 2 in this case represents slope and intercept (2 regression parameters)
  eta <- kf_out$xtt1 #one step ahead forecast of regr parameters calc with kalman filter, represented as xtt1 in marss notation


  ## variance of regr parameters (slope and intercept); 1x2xT array
  Phi <- kf_out$Vtt1

  #Since DLM is time-varying only the last values in the training data are used to forecast the next year
  # Get the last forecasted regression parameters
  last_eta <- eta[, ncol(eta)]

  # Get the last forecasted variance
  last_Phi <- Phi[, , ncol(Phi)]

  #get index value for next year and standardize
  test_value<-test.data$value


  #get mean and standard deviation of index used in model
  mean_index <- mean(index, na.rm = TRUE)
  sd_index <- sd(index, na.rm = TRUE)
  test_z_value <- (test_value - mean_index) / sd_index  #standardize using the train dataset mean and standard deviation



  #forecast logit.s using last predicted regression parameters and z-value for next year
  forecasted_logit_s <- last_eta[1] + last_eta[2] * test_z_value


  # Calculate the forecasted standard error
  forecasted_se <- sqrt(last_Phi[1, 1] + 2 * last_eta[2] * last_Phi[1, 2] + last_eta[2]^2 * last_Phi[2, 2])

  #forecasted_s <- exp(forecasted_logit_s) / (1 + exp(forecasted_logit_s))*100
  forecasted_s_plogis <- plogis(forecasted_logit_s)*100

  # Calculate the prediction interval
  z_value<-qnorm(.975) ## Z-value for 95% confidence interval ~1.96 (change as needed)
  lower <- forecasted_s_plogis - z_value * forecasted_se
  upper <- forecasted_s_plogis + z_value * forecasted_se

  #create dataframe for forecast
  df_nextyear<- data.frame(
    y = NA,
    estimate = forecasted_s_plogis,
    fore_CI_95_lower = lower,
    fore_CI_95_upper = upper
  )

  #extract predictions for all years --currently using forecast but should be using predict--doesn't seem to work by calling MARSS::predict()--could try manually extracting and calculating predictions
  forecast_df<-MARSS::forecast(dlm, h= 1, type = "ytT", interval = "confidence")

  
    # Check if 'logit.s' and 'value' are empty--use MARSS step-ahead forecast without a index value if it is, if not empty, use test.data index value to forecast nahead
  if (is.na(test.data$logit.s) && is.na(test.data$value)) {
    # If both are empty, forecast with h=1 and keep last estimate
      df_pred_raw<- forecast_df$pred
    
     #wrangle for plot
    df_pred<-df_pred_raw%>%
    dplyr::mutate(dplyr::across(c(,3:9), ~stats::plogis(.) * 100)) %>%
    dplyr::select(y, estimate, "fore_CI_95_lower" = `Lo 95`,"fore_CI_95_upper" = `Hi 95`)

  results<-  df_pred %>%
    dplyr::mutate(year = (min(years):max(years+1)),
                 index = unique(data$index),
                 sar.method = unique(data$sar.method),
                 dataset = "base_forecast",
                  rear_type = "Natural-origin",
                  pass_type = "All") %>%
    dplyr::inner_join(dplyr::select(data, value, year), by = "year") %>% ##join index value
    dplyr::select(year, value, y, estimate, fore_CI_95_lower, fore_CI_95_upper,sar.method,index, rear_type, pass_type, dataset)
    
    return(results)
  } else {
  #remove last row (incorrect forecast for next year)
  df_pred_raw<- forecast_df$pred  %>% head(-1)


  #wrangle for plot
  df_pred<-df_pred_raw%>%
    dplyr::mutate(dplyr::across(c(,3:9), ~stats::plogis(.) * 100)) %>%
    dplyr::select(y, estimate, "fore_CI_95_lower" = `Lo 95`,"fore_CI_95_upper" = `Hi 95`)

  results<-  df_pred %>%
    dplyr::bind_rows(df_nextyear) %>%
    dplyr::mutate(year = (min(years):max(years+1)),
                 index = unique(data$index),
                 sar.method = unique(data$sar.method),
                 dataset = "base_forecast",
                  rear_type = "Natural-origin",
                  pass_type = "All") %>%
    dplyr::inner_join(dplyr::select(data, value, year), by = "year") %>% ##join index value
    dplyr::select(year, value, y, estimate, fore_CI_95_lower, fore_CI_95_upper,sar.method,index, rear_type, pass_type, dataset)


  return(results)
  }
}
```



```{r}
# Get the unique combinations of index and sar.method
combinations <- unique(sar_raw_data[, c("index", "sar.method")])


# Initialize an empty list to store the forecasts
forecasts <- list()

# Loop over each combination
for(i in seq_len(nrow(combinations))) {
  # Subset the data for the current combination
  data_subset <- sar_raw_data[sar_raw_data$index == combinations$index[i] & 
                                      sar_raw_data$sar.method == combinations$sar.method[i], ]
  
  # Run the model and get the forecast
  forecast <- run_model_and_forecast(data_subset)
  
  # Store the forecast in the list
  forecasts[[i]] <- forecast
}

# Combine all forecasts into one data frame
all_forecasts <- do.call(rbind, forecasts)


# check
all_forecasts %>% 
  group_by(sar.method, index) %>% 
  summarise(n_distinct(year))

base_plot_data<-all_forecasts

#save updated sar_raw_data
usethis::use_data(base_plot_data, overwrite = TRUE)

```








```{r}
data <- sar_raw_data_updated %>% 
  filter(index == "CUI", 
         sar.method == "Scheuerell and Williams (2005)")

fct_model_single<-function(data){
 years <- data$year
  ## number of years of data
  TT <- length(years)
  ## get response variable: logit(survival)
  dat <- matrix(data$logit.s, nrow = 1)

  ## get predictor variable
  index <- data$value

  ## ## ## z-score the upwelling index
  index_z <- matrix((index - mean(index)) / sqrt(var(index)), nrow = 1)

  # index_z_train <- index_z[1:TT]

  ## number of regr params (slope + intercept) = 2
  m <- dim(index_z)[1] + 1


  ### build DLM

  ## for process eqn
  B <- diag(m)                        ## 2x2; Identity
  U <- matrix(0, nrow = m, ncol = 1)  ## 2x1; both elements = 0
  Q <- matrix(list(0), m, m)          ## 2x2; all 0 for now
  diag(Q) <- c("q.alpha", "q.beta")   ## 2x2; diag = (q1,q2)

  ## for observation eqn
  Z <- array(NA, c(1, m, TT))  ## NxMxT; empty for now
  Z[1,1,] <- rep(1, TT)        ## Nx1; 1's for intercept
  Z[1,2,] <- index_z             ## Nx1; predictor variable
  A <- matrix(0)               ## 1x1; scalar = 0
  R <- matrix("r")             ## 1x1; scalar = r

  ## only need starting values for regr parameters
  inits_list <- list(x0 = matrix(c(0, 0), nrow = m))

  ## list of model matrices & vectors
  mod_list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R)


  ### fit DLM
  # set.seed(1234)
  ## fit univariate DLM
  dlm_train <- MARSS::MARSS(dat, inits = inits_list, model = mod_list)


  # #pull indice with full data z score
  # index_z_test <- index_z[(TT+1)] #get 1 CUI ahead

  # #newdata z array
  # Z_test <- array(NA, c(1, m, 3))  ## NxMxT; empty for now
  # Z_test[1,1,] <- rep(1, 3)        ## Nx1; 1's for intercept
  # Z_test[1,2,] <- index_z_test

  ## forecast
  forecast_df<-MARSS::forecast(dlm_train, h = 1, type = "ytT", interval = "confidence")
  
  return(list (
    dlm_train = dlm_train,
    mod_list = mod_list)
    )

}

results<-fct_model_single(data)


# Perform a Kalman filter on the model
kf <- MARSS::MARSSkf(results$dlm_train)

# Extract the one-step ahead in-sample predictions
predictions <- kf$ytT


# Convert the predictions to the same scale as the original data
predictions_scaled <-plogis(predictions) * 100

# Compare the predictions to the actual values
comparison <- data.frame(
  Year = train_data$year,
  Actual = train_data$logit.s,
  Predicted = predictions_scaled
)

print(comparison)


# Extract the filtered state estimates
state_estimates <- kf$xtt1
state_estimates <- matrix(state_estimates, ncol = 1)


# Calculate the one-step ahead predictions
predictions <- results$mod_list$Z %*% state_estimates + results$mod_list$A

# Convert the predictions to the same scale as the original data
predictions_scaled <- stats::plogis(predictions) * 100

```
 
